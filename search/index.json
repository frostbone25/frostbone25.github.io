[{"content":"By: David Matos\nSharing personal my notes, and experiments with optimizing my real-time Global Illumination lighting buffers. The main goal is to reduce the rendering cost while maximizing final image quality and stability.\nJapanese Urban City Night Pack Environment in Unity\nTable of Contents Preface Upsampling Upsampling - Chain Setup Upsampling - The Basics (Point, Bilinear) Upsampling - Wider Blur Filters Upsampling - Depth Aware Upsampling - Depth and Normal Aware Temporal Instabilities Temporal Instability Solutions: Temporal Filtering? Investigations into Temporal Instabilities Temporal Instability Solutions: Downsampling? Downsampling - Drawbacks Temporal Solution Comparisons Upsampling - How to go further? Spherical Harmonics Spherical Harmonics: How to apply it? Spherical Harmonic Optimizations: Compute Shader Spherical Harmonic Optimizations: Render Target Packing Spherical Harmonic Bonuses How to go even further and beyond? References / Sources Preface In a Real-Time rendering context, when computing any image-space effects, it\u0026rsquo;s very common as a performance measure to never compute those effects at full screen resolution. The reason for this is simple, the more pixels we have, the more work we do.\nSay our final target resolution is 1920 x 1080. Using this resolution will equate to\u0026hellip; 1920 x 1080 = 2,073,600 pixels\nThat is a lot of pixels! Now what you need to consider is that when doing things at this target resolution (for each of these 2 million pixels) you are going to run a set of shader instructions/operations for every pixel, for the given effect.\nFor some light-weight effects like bloom, or tonemapping the amount of overall work that you\u0026rsquo;ll be doing per pixel is fairly small.\nHowever for more complicated effects like SSAO (Screen-Space Ambient Occlusion), SSR (Screen Space Reflections), or full on lighting buffers for Global Illumination (either using Ray-Tracing, Voxel Cone-tracing, etc.) The amount of overall work needed for those effects is significant.\nFull Resolution Global Illumination (Voxel Cone Tracing)\nThe end result is that for a large amount of total pixels, you end up doing a large amount of work and your overall performance plummets.\nYou can cut back on the amount of work you do per pixel, which is a viable strategy. Unfortunately though with most of these effects/algorithims you can only simplify it so much. The work is required in order to do the effect in the first place.\nSo if you can\u0026rsquo;t reduce the amount of work you do per-pixel, you can reduce the amount of pixels that we do work for in the first place. It\u0026rsquo;s the easiest and also the most effective strategy.\nAgain our target resolution is 1920 x 1080 = 2,073,600 pixels. If we reduce the target resolution by half we wind up with the following\u0026hellip;\n1 2 3 4 5 Full Width: 1920 Full Height: 1080 Half Width: (1920 / 2) = 960 Half Height: (1080 / 2) = 540 Final Resolution: 960 x 540 = 518,400 pixels Down from 2 million pixels, to just 500 thousand. That is a pretty nice cost saving! We cut down the amount of work we were doing by half! What happens if we go further by going down a quarter?\n1 2 3 4 5 6 7 Full Width: 1920 Full Height: 1080 Half Width: (1920 / 2) = 960 Half Height: (1080 / 2) = 540 Quarter Width: (960 / 2) = 480 Quarter Height: (540 / 2) = 270 Final Resolution: 480 x 270 = 129,600 pixels Sweet! From 500 thousand to now just a little over a 100 thousand pixels. That is a massive reduction, factor in the work needed for every pixel we end up with a pretty significant reduction in our final workload needed for the effect.\n(NOTE: I would like to point out that there are definetly a lot of over-generalizations that is happening here with this example. Obviously there are some caveats and edge cases as always\u0026hellip; but the concept remains the same. The less work you have to do, the better your performance).\nHowever\u0026hellip; as attractive as the numbers and instruction counts come out to be, this does come with it\u0026rsquo;s own set of tradeoffs.\nRaw 1/4th Resolution Diffuse Global Illumination (Voxel Cone Tracing)\nOptimization is a game of tradeoffs, and reducing resolution will introduce some problems\u0026hellip;\nIncrease the size of pixels, which means increased aliasing/pixelation artifacts. Depending on the effect, misalignment of parts of the image no longer align fully with the original. Decreased temporal stability with the pixels being much larger, so the more the final pixel value changes between frames the more flickering and instability occurs. Yikes!\nIf your effects don\u0026rsquo;t need to utilize scene depth/normal or any other related buffers this might be fine. But for my case where I\u0026rsquo;m calculating a lighting buffer that large parts of my project will be lit by\u0026hellip; this is not good at all. We can\u0026rsquo;t use this as is, so we need to do some work in order to make it usable.\nBut wait\u0026hellip; didn\u0026rsquo;t we also mention that we were trying to reduce the workload in the first place per pixel? Yeah\u0026hellip; and this is where the true difficulties of optimization come in. It\u0026rsquo;s a game of tradeoffs, so you need to be careful when doing the work needed to make something usable, because you might end up doing more work in the end than it was to just render the effect at a better resolution in the first place!\nSo what can we do? Well fortunately\u0026hellip; the industry has come up with many solutions that we can employ here to squeeze some more juice out of these shrunken fruits without exploding the final costs.\nUpsampling The basic idea with upsampling in this context is to \u0026ldquo;cheat\u0026rdquo; by essentially creating more data, without actually calculating more data. In our case with images we are trying to create more pixels, without actually calculating all of those new pixels (At least not using the original heavy way to calculate those pixels, because it was expensive to begin with!).\nIt\u0026rsquo;s also important to note here that we will be covering classic upsampling methods that exist, not machine learning-based techniques like DLSS, FSR, Intel XeSS, etc.\nWhile the end goal is the same, those technologies are aimed at upsampling a final rendered image (not an image within the pipeline that is to help with the final rendered image), and are usually quite heavy.\nUpsampling - Chain Setup I would also like to preface that these upsamples happen in multiple stages, it is not one single pass. This is by design and intentional in order to maximize pixel coverage and performance.\nIf you have done blur effects, you understand that it\u0026rsquo;s common to do blur effects in two passes. For example if your doing a wide blur that spans across 32 pixels in each direction, you do it in two passes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 //Blur Pass 1 (Horizontal Pass) for(int i = 0; i \u0026lt; 32; i++) { //spread pixels around horizontally } //|||||||||||||||||||||||||||||||||||||||||| //Blur Pass 2 (Vertical Pass) for(int i = 0; i \u0026lt; 32; i++) { //reusing the previous horizontal blurred render target //spread pixels around vertically... } //|||||||||||||||||||||||||||||||||||||||||| //End Result //(32 + 32) = 64 samples Versus in a single pass, the amount of samples you end up doing is multiplicative\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //Single-Pass Blur for(int x = 0; x \u0026lt; 32; x++) { for(int y = 0; y \u0026lt; 32; y++) { float2 offset = float2(x, y) * TexelSize; //spread pixels around horizontally and vertically... } } //|||||||||||||||||||||||||||||||||||||||||| //End Result //(32 x 32) = 1024 samples!!! While the upsampling stages are not structured like this exactly, the concept is similar in that we do a relatively small amount of work in a pass. Then this work gets reused again in the next pass, and we do the same thing again spreading things around more but just at a different resolution.\nAlso, because we are operating on a different resolution this affects our scale since we are working per pixel/texel (Lower resolution means we effectively spread pixels around at a larger scale, higher resolution means we spread pixels around at a smaller scale).\nVersus just 1 singular pass where we have to span across many pixel regions in order to get the same result, doing potentially thousands or more samples per pixel to achieve the same result, just like with the single pass blur example.\nEach upsample pass/stage is essentially split by a power of two.\nFull Target Screen Resolution (No Upsample Passes) 1/2th Target Screen Resolution (1 Upsample Pass) 1/4th Target Screen Resolution (2 Upsample Passes) 1/8th Target Screen Resolution (3 Upsample Passes) 1/16th Target Screen Resolution (4 Upsample Passes) 1/32th Target Screen Resolution (5 Upsample Passes) In practice for 1/4th Resolution\u0026hellip;\n(Inital Effect) Calculate effect at 1/4th resolution (Upsample Stage 1) Upsample from 1/4th -\u0026gt; 1/2th (Upsample Stage 2) Upsample from 1/2th -\u0026gt; Full Resolution All upsampling related segments utilize this chaining process going forward.\nNow lets explore what I did in regards to how I spread those pixels around\u0026hellip;\nUpsampling - The Basics (Point, Bilinear) The most basic-basic upsampling we can do is just to duplicate/copy the original pixel we have into the neighboring regions (via Point Filtering).\n1/4th Resolution (2 upsample passes)\nWe can see this does not look good at all for what we ultimately want to achieve. Point filtering just simply copies the data we have into more regions, but it does not look good here.\nWe get visible blocking and pixelation, and the colors themselves are not properly aligned with the scene like it is with full resolution.\nThis can be improved by using bilinear filtering which will gradually \u0026ldquo;fade\u0026rdquo; between one pixel, and the next within a 2x2 pixel region.\n1/4th Resolution (2 upsample passes)\nIt\u0026rsquo;s an improvement\u0026hellip; It\u0026rsquo;s certianly more desirable than the point filtering, but the blockiness is still very visible. Additionally the colors themselves are not properly aligned with the scene like it was with full resolution.\nBilinear filtering does help (and the cost is essentially free, virtually all modern hardware now supports it) but obviously it\u0026rsquo;s not enough\u0026hellip;\nWe need to cover a wider pixel region!\nUpsampling - Wider Blur Filters 1/4th Resolution (2 upsample passes)\nHere I am showing off a 9-tap single-pass gaussian blur.\nWe can see that now the underlying effect is much smoother compared to bilinear filtering. The pixelation artifacts and blockiness are much less visible now. Awesome!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 float4 UpsampleGaussian9Tap(float2 uv, float2 texelSize) { float4 uvOffset = texelSize.xyxy * float4(1.0f, 1.0f, -1.0f, 0.0f); half4 color = LowResColorSample(uv - uvOffset.xy); color += LowResColorSample(uv - uvOffset.wy) * 2.0f; color += LowResColorSample(uv - uvOffset.zy); color += LowResColorSample(uv + uvOffset.zw) * 2.0f; color += LowResColorSample(uv) * 4.0f; color += LowResColorSample(uv + uvOffset.xw) * 2.0f; color += LowResColorSample(uv + uvOffset.zy); color += LowResColorSample(uv + uvOffset.wy) * 2.0f; color += LowResColorSample(uv + uvOffset.xy); return color * 0.0625f; } I am using a 9-tap single-pass gaussian blur. Not multi-pass, just single pass but at a very small radius. You could use different variants like a box filter, or a smaller kernel like a 5-tap or a 7-tap hexagonal.\nBut of course while we did mitigate most of the pixelation/blockiness, we still are left with the other problem with doing things at low resolution.\nThat being that piels do not align with the underlying scene well at all. The wider filtering we are doing here just looks like a wierd bloom or blur effect. None of the details of the scene are preserved and it will only get worse with lower and lower resolutions.\n1/8th Resolution (3 upsample passes)\nWe need to somehow more intelligently determine what spots will recieve more blur, and what spots will try to remain sharp.\nSo how can we do this?\nUpsampling - Depth Aware Enter depth upsampling.\nWe build on the idea before where we need to spread our pixel across to more areas than just that 2x2 block. This fixes the pixelation/aliasing problems, but we need to be smarter as to how to spread those pixels around.\nFortunately, due to the nature of this effect (Diffuse Global Illumination) we already have access to a depth buffer which we used initally to calculate lighting for the visible surfaces in the scene.\nRaw Scene Depth\nThe depth buffer essentially tells us where surfaces are in the scene. Let\u0026rsquo;s reuse it here to guide our upsampling filter!\nUsing the depth buffer we can determine roughly where there are continous smooth/flat surfaces, or if there are large gaps/discontinuities.\nThe left side is zoomed into a pixel region of the depth buffer where we can see that alot of the pixels here are roughly the same. So when we compute the depth differences between the pixels in this region, it will be small or the same. This means that we should spread pixels around here more easily.\nThe right side is zoomed into a pixel region of the depth buffer where we can see that there a lot of differences and discontinuity. So when we compute the depth differences between the pixels in this region, it will be large. This means that we should NOT spread pixels around here that easily.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 float4 UpsampleGaussian9TapDepthAware(float2 uv, float2 texelSize) { float highResDepth = HighResDepthSample(uv); float2 uv0 = uv + float2(-texelSize.x, -texelSize.y); //top left float2 uv1 = uv + float2( 0.0, -texelSize.y); //top float2 uv2 = uv + float2( texelSize.x, -texelSize.y); //top right float2 uv3 = uv + float2(-texelSize.x, 0.0); //left float2 uv5 = uv + float2( texelSize.x, 0.0); //right float2 uv6 = uv + float2(-texelSize.x, texelSize.y); //bottom left float2 uv7 = uv + float2( 0.0, texelSize.y); //bottom float2 uv8 = uv + float2( texelSize.x, texelSize.y); //bottom right float lowResDepth0 = LowResDepthSample(uv0); float lowResDepth1 = LowResDepthSample(uv1); float lowResDepth2 = LowResDepthSample(uv2); float lowResDepth3 = LowResDepthSample(uv3); float lowResDepth4 = LowResDepthSample(uv); //center float lowResDepth5 = LowResDepthSample(uv5); float lowResDepth6 = LowResDepthSample(uv6); float lowResDepth7 = LowResDepthSample(uv7); float lowResDepth8 = LowResDepthSample(uv8); float depthWeight0 = (1.0 / (abs(highResDepth - lowResDepth0) + 1e-4)); float depthWeight1 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth1) + 1e-4)); float depthWeight2 = (1.0 / (abs(highResDepth - lowResDepth2) + 1e-4)); float depthWeight3 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth3) + 1e-4)); float depthWeight4 = 4.0 * (1.0 / (abs(highResDepth - lowResDepth4) + 1e-4)); //center float depthWeight5 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth5) + 1e-4)); float depthWeight6 = (1.0 / (abs(highResDepth - lowResDepth6) + 1e-4)); float depthWeight7 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth7) + 1e-4)); float depthWeight8 = (1.0 / (abs(highResDepth - lowResDepth8) + 1e-4)); float4 color0 = LowResColorSample(uv0); float4 color1 = LowResColorSample(uv1); float4 color2 = LowResColorSample(uv2); float4 color3 = LowResColorSample(uv3); float4 color4 = LowResColorSample(uv); //center float4 color5 = LowResColorSample(uv5); float4 color6 = LowResColorSample(uv6); float4 color7 = LowResColorSample(uv7); float4 color8 = LowResColorSample(uv8); float4 color = color0 * depthWeight0 + color1 * depthWeight1 + color2 * depthWeight2 + color3 * depthWeight3 + color4 * depthWeight4 + color5 * depthWeight5 + color6 * depthWeight6 + color7 * depthWeight7 + color8 * depthWeight8; float totalWeight = depthWeight0 + depthWeight1 + depthWeight2 + depthWeight3 + depthWeight4 + depthWeight5 + depthWeight6 + depthWeight7 + depthWeight8; return color / max(totalWeight, 1e-6); //avoid divide by zero with max } By using Depth upsampling here and weighting (multiplying) each of our taps based on how the depth changes, we can see how this affects our blur filter now\u0026hellip;\n1/4th Resolution (2 upsample passes)\nWe can actually see some of original sharp edges of some of the objects in the scene coming back now!\nFor some effects like volumetric fog, or mabye even SSAO, this is usually where your work would end.\nBut with something as complicated as calulating diffuse global illumination however, I discovered that this still wasn\u0026rsquo;t quite convincing enough for me (and mabye even for you).\nThe quality is certainly better than it was, but there were still large portions of the scene here where surfaces that originally had plenty of detail now get smeared over when we compare with the original expensive full resolution buffer.\nFull Resolution (0 upsample passes)\nSo using depth helps, but it looks like we need more information about the scene in order to better spread our pixels around. Is there another attribute aside from just depth that we can utlize to help align/place pixels even more?\nUpsampling - Depth and Normal Aware Enter depth-normal upsampling.\nOnce again we build on the idea before with depth aware upsampling. Now we add an additional attribute, so along with using the scene depth buffer, we will also use scene normal buffer to help more with retaining the edges of objects and even some normal mapped details.\nJust like the depth buffer, we also used the normal buffer in our inital lighting calculations to find the orientation of a surface and calculate the lighting of that point. Lets reuse it again here to guide our upsample filtering.\nScene Normals ([0..1] range)\nNormals tell us the orientation of surfaces in a scene.\nThe left side is zoomed into a pixel region of the normal buffer where we can see that alot of the pixels here are roughly the same. So when we compute the normal differences between the pixels in this region, it will be small or the same. This means that we should spread pixels around here more easily.\nThe right side is zoomed into a pixel region of the normal buffer where we can see that there a lot of differences and discontinuity. So when we compute the normal differences between the pixels in this region, it will be large. This means that we should NOT spread pixels around here that easily.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 float4 UpsampleSmooth_Gaussian9Tap_DepthNormalAwareInterpolated(float2 uv, float2 texelSize) { float highResDepth = HighResDepthSample(uv); float3 highResNormal = HighResNormalSample(uv); float2 uv0 = uv + float2(-texelSize.x, -texelSize.y); //top left float2 uv1 = uv + float2( 0.0, -texelSize.y); //top float2 uv2 = uv + float2( texelSize.x, -texelSize.y); //top right float2 uv3 = uv + float2(-texelSize.x, 0.0); //left float2 uv5 = uv + float2( texelSize.x, 0.0); //right float2 uv6 = uv + float2(-texelSize.x, texelSize.y); //bottom left float2 uv7 = uv + float2( 0.0, texelSize.y); //bottom float2 uv8 = uv + float2( texelSize.x, texelSize.y); //bottom right float lowResDepth0 = LowResDepthSample(uv0); float lowResDepth1 = LowResDepthSample(uv1); float lowResDepth2 = LowResDepthSample(uv2); float lowResDepth3 = LowResDepthSample(uv3); float lowResDepth4 = LowResDepthSample(uv); float lowResDepth5 = LowResDepthSample(uv5); float lowResDepth6 = LowResDepthSample(uv6); float lowResDepth7 = LowResDepthSample(uv7); float lowResDepth8 = LowResDepthSample(uv8); float3 lowResNormal0 = LowResNormalSample(uv0); float3 lowResNormal1 = LowResNormalSample(uv1); float3 lowResNormal2 = LowResNormalSample(uv2); float3 lowResNormal3 = LowResNormalSample(uv3); float3 lowResNormal4 = LowResNormalSample(uv); float3 lowResNormal5 = LowResNormalSample(uv5); float3 lowResNormal6 = LowResNormalSample(uv6); float3 lowResNormal7 = LowResNormalSample(uv7); float3 lowResNormal8 = LowResNormalSample(uv8); float depthWeight0 = (1.0 / (abs(highResDepth - lowResDepth0) + 1e-4)); float depthWeight1 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth1) + 1e-4)); float depthWeight2 = (1.0 / (abs(highResDepth - lowResDepth2) + 1e-4)); float depthWeight3 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth3) + 1e-4)); float depthWeight4 = 4.0 * (1.0 / (abs(highResDepth - lowResDepth4) + 1e-4)); float depthWeight5 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth5) + 1e-4)); float depthWeight6 = (1.0 / (abs(highResDepth - lowResDepth6) + 1e-4)); float depthWeight7 = 2.0 * (1.0 / (abs(highResDepth - lowResDepth7) + 1e-4)); float depthWeight8 = (1.0 / (abs(highResDepth - lowResDepth8) + 1e-4)); //NOTE: this might be able to be approximated or simplified some more float normalPower = 10.0f; float normalWeight0 = pow(saturate(dot(highResNormal, lowResNormal0)), normalPower); float normalWeight1 = pow(saturate(dot(highResNormal, lowResNormal1)), normalPower); float normalWeight2 = pow(saturate(dot(highResNormal, lowResNormal2)), normalPower); float normalWeight3 = pow(saturate(dot(highResNormal, lowResNormal3)), normalPower); float normalWeight4 = pow(saturate(dot(highResNormal, lowResNormal4)), normalPower); float normalWeight5 = pow(saturate(dot(highResNormal, lowResNormal5)), normalPower); float normalWeight6 = pow(saturate(dot(highResNormal, lowResNormal6)), normalPower); float normalWeight7 = pow(saturate(dot(highResNormal, lowResNormal7)), normalPower); float normalWeight8 = pow(saturate(dot(highResNormal, lowResNormal8)), normalPower); float4 color0 = LowResColorSample(uv0); float4 color1 = LowResColorSample(uv1); float4 color2 = LowResColorSample(uv2); float4 color3 = LowResColorSample(uv3); float4 color4 = LowResColorSample(uv); float4 color5 = LowResColorSample(uv5); float4 color6 = LowResColorSample(uv6); float4 color7 = LowResColorSample(uv7); float4 color8 = LowResColorSample(uv8); //combined weights float w0 = depthWeight0 * normalWeight0; float w1 = depthWeight1 * normalWeight1; float w2 = depthWeight2 * normalWeight2; float w3 = depthWeight3 * normalWeight3; float w4 = depthWeight4 * normalWeight4; float w5 = depthWeight5 * normalWeight5; float w6 = depthWeight6 * normalWeight6; float w7 = depthWeight7 * normalWeight7; float w8 = depthWeight8 * normalWeight8; //weighted accumulation float4 color = color0 * w0 + color1 * w1 + color2 * w2 + color3 * w3 + color4 * w4 + color5 * w5 + color6 * w6 + color7 * w7 + color8 * w8; float totalWeight = w0 + w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8; color /= max(totalWeight, 1e-6); return color; } Implementing it very similarly here with the depth, we just do 9 taps (or whatever amount your underlying filter is doing) with normal, then calculate the deviations that happen between the pixel regions and we weigh these with our depth.\n1/4th Resolution (2 upsample passes)\nNice! We can see now that this is a significant improvement over just depth-aware upsampling. Geometry edges are almost fully preserved now, and much less pixelation artifacts are visible at the same time.\nIn fact this gives me enough confidence in that I might push things further by dropping the resolution to 1/8th and see how things hold up.\n1/8th Resolution (3 upsample passes)\nSurpisingly well, some pixelation is more visible now of course but the depth-normal aware upsampling is doing a fantastic job of keeping all of the geometry edges and some of the normals sharp and visible.\nBut even then\u0026hellip; there are still some things that irk me even at 1/4th resolution, and the issues get more visible when dropping down to 1/8th now. Geometry edges are kept in-tact but most of the original normal-mapped material details of the scene are mostly gone now. Pixelation details also are starting to become even more visible.\nIs there a way to hide those pixelation artifacts some more, and mabye even bring back normal-mapped details?\nSpoiler: Yes!\nOptimization Bonus Now I would like to point out here that depending on your setup this depth-normal upsampling will cost an extra render target, which means more texture lookups to factor in this normal buffer read like you see in this implementation.\nAn additional optimization here that you can that is to pack a singluar float4 render target with both depth (16-bit half precision) and normal (encoded into a 2 channel using octahedron/spherical coordinates/etc).\nRG: Depth (Half Precsion 16-bit) BA: Normal (Encoded into 2 component 16-bit) This would reduce the texture fetches, at the expense of adding additional instructions for unpacking the render target for use in the filter. (You are basically trading memory for ALU here. Fortunately with most modern hardware now being more capable, this is prefered so that way you are not bound by memory access speeds with texture reads).\n1 2 3 4 5 6 7 8 9 10 11 //PUSEDO CODE void UnpackDepthNormalTarget(in float4 depthNormal, out float depth, out float3 normal) { //take \u0026#39;depthNormal\u0026#39; which should be RGBA32 (or RGBA64 if you want more precision) //RG: 16 bits half precision depth //BA: 16 bits encoded normal (normal can be encoded with spherical coordinates/octahedron) //decode and output depth... //decode and output normal... } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 //optimizations made // + texture fetches are reduced by 10 (19 now, compared to 29 before) // achieved with a packed RGBA Depth Normal render target. // trading for more ALU for unpacking instructions // + re-wrote normal weights and replaced pow with 4 mul\u0026#39;s (hardcoded pow 4.0) // + re-wrote depth weights to use HLSL rcp() float4 UpsampleSmooth_Gaussian9Tap_DepthNormalAwareInterpolatedOptimized(float2 uv, float2 texelSize) { float4 highResDepthNormal = HighResDepthNormalSample(uv); float highResDepth; float3 highResNormal; UnpackDepthNormalTarget(highResDepthNormal, highResDepth, highResNormal); float2 uv0 = uv + float2(-texelSize.x, -texelSize.y); //top left float2 uv1 = uv + float2( 0.0, -texelSize.y); //top float2 uv2 = uv + float2( texelSize.x, -texelSize.y); //top right float2 uv3 = uv + float2(-texelSize.x, 0.0); //left float2 uv5 = uv + float2( texelSize.x, 0.0); //right float2 uv6 = uv + float2(-texelSize.x, texelSize.y); //bottom left float2 uv7 = uv + float2( 0.0, texelSize.y); //bottom float2 uv8 = uv + float2( texelSize.x, texelSize.y); //bottom right float4 lowResDepthNormal0 = LowResDepthNormalSample(uv0); float4 lowResDepthNormal1 = LowResDepthNormalSample(uv1); float4 lowResDepthNormal2 = LowResDepthNormalSample(uv2); float4 lowResDepthNormal3 = LowResDepthNormalSample(uv3); float4 lowResDepthNormal4 = LowResDepthNormalSample(uv); float4 lowResDepthNormal5 = LowResDepthNormalSample(uv5); float4 lowResDepthNormal6 = LowResDepthNormalSample(uv6); float4 lowResDepthNormal7 = LowResDepthNormalSample(uv7); float4 lowResDepthNormal8 = LowResDepthNormalSample(uv8); float lowResDepth0; float lowResDepth1; float lowResDepth2; float lowResDepth3; float lowResDepth4; float lowResDepth5; float lowResDepth6; float lowResDepth7; float lowResDepth8; float3 lowResNormal0; float3 lowResNormal1; float3 lowResNormal2; float3 lowResNormal3; float3 lowResNormal4; float3 lowResNormal5; float3 lowResNormal6; float3 lowResNormal7; float3 lowResNormal8; UnpackDepthNormalTarget(lowResDepthNormal0, lowResDepth0, lowResNormal0); UnpackDepthNormalTarget(lowResDepthNormal1, lowResDepth1, lowResNormal1); UnpackDepthNormalTarget(lowResDepthNormal2, lowResDepth2, lowResNormal2); UnpackDepthNormalTarget(lowResDepthNormal3, lowResDepth3, lowResNormal3); UnpackDepthNormalTarget(lowResDepthNormal4, lowResDepth4, lowResNormal4); UnpackDepthNormalTarget(lowResDepthNormal5, lowResDepth5, lowResNormal5); UnpackDepthNormalTarget(lowResDepthNormal6, lowResDepth6, lowResNormal6); UnpackDepthNormalTarget(lowResDepthNormal7, lowResDepth7, lowResNormal7); UnpackDepthNormalTarget(lowResDepthNormal8, lowResDepth8, lowResNormal8); //replaced (1.0 / difference) with HLSL rcp() float depthWeight0 = rcp(abs(highResDepth - lowResDepth0) + 1e-4); float depthWeight1 = 2.0 * rcp(abs(highResDepth - lowResDepth1) + 1e-4); float depthWeight2 = rcp(abs(highResDepth - lowResDepth2) + 1e-4); float depthWeight3 = 2.0 * rcp(abs(highResDepth - lowResDepth3) + 1e-4); float depthWeight4 = 4.0 * rcp(abs(highResDepth - lowResDepth4) + 1e-4); float depthWeight5 = 2.0 * rcp(abs(highResDepth - lowResDepth5) + 1e-4); float depthWeight6 = rcp(abs(highResDepth - lowResDepth6) + 1e-4); float depthWeight7 = 2.0 * rcp(abs(highResDepth - lowResDepth7) + 1e-4); float depthWeight8 = rcp(abs(highResDepth - lowResDepth8) + 1e-4); //NOTE: this might be able to be approximated or simplified even more float normalWeight0 = saturate(dot(highResNormal, lowResNormal0)); float normalWeight1 = saturate(dot(highResNormal, lowResNormal1)); float normalWeight2 = saturate(dot(highResNormal, lowResNormal2)); float normalWeight3 = saturate(dot(highResNormal, lowResNormal3)); float normalWeight4 = saturate(dot(highResNormal, lowResNormal4)); float normalWeight5 = saturate(dot(highResNormal, lowResNormal5)); float normalWeight6 = saturate(dot(highResNormal, lowResNormal6)); float normalWeight7 = saturate(dot(highResNormal, lowResNormal7)); float normalWeight8 = saturate(dot(highResNormal, lowResNormal8)); //NOTE: power has been reduced to 4, so the normal effect may not appear as strong but saves you from doing a pow op //you could just stack more mul\u0026#39;s if you wanted a stronger effect normalWeight0 *= normalWeight0 * normalWeight0 * normalWeight0; //hardcoded pow 4.0 normalWeight1 *= normalWeight1 * normalWeight1 * normalWeight1; //hardcoded pow 4.0 normalWeight2 *= normalWeight2 * normalWeight2 * normalWeight2; //hardcoded pow 4.0 normalWeight3 *= normalWeight3 * normalWeight3 * normalWeight3; //hardcoded pow 4.0 normalWeight4 *= normalWeight4 * normalWeight4 * normalWeight4; //hardcoded pow 4.0 normalWeight5 *= normalWeight5 * normalWeight5 * normalWeight5; //hardcoded pow 4.0 normalWeight6 *= normalWeight6 * normalWeight6 * normalWeight6; //hardcoded pow 4.0 normalWeight7 *= normalWeight7 * normalWeight7 * normalWeight7; //hardcoded pow 4.0 normalWeight8 *= normalWeight8 * normalWeight8 * normalWeight8; //hardcoded pow 4.0 float4 color0 = LowResColorSample(uv0); float4 color1 = LowResColorSample(uv1); float4 color2 = LowResColorSample(uv2); float4 color3 = LowResColorSample(uv3); float4 color4 = LowResColorSample(uv); float4 color5 = LowResColorSample(uv5); float4 color6 = LowResColorSample(uv6); float4 color7 = LowResColorSample(uv7); float4 color8 = LowResColorSample(uv8); //combined weights float w0 = depthWeight0 * normalWeight0; float w1 = depthWeight1 * normalWeight1; float w2 = depthWeight2 * normalWeight2; float w3 = depthWeight3 * normalWeight3; float w4 = depthWeight4 * normalWeight4; float w5 = depthWeight5 * normalWeight5; float w6 = depthWeight6 * normalWeight6; float w7 = depthWeight7 * normalWeight7; float w8 = depthWeight8 * normalWeight8; //weighted accumulation float4 color = color0 * w0 + color1 * w1 + color2 * w2 + color3 * w3 + color4 * w4 + color5 * w5 + color6 * w6 + color7 * w7 + color8 * w8; float totalWeight = w0 + w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8; color /= max(totalWeight, 1e-6); return color; } Temporal Instabilities Now, quick spoiler there is still a couple of more additional stages needed to improve the quality even more but I wanted to take a pause real quick because there is something very important here that we need to consider before moving forward.\nRemember that context wise we are in a game setting (or a real-time context). The camera will be moving and changing, so will the scene\u0026hellip; so how does our work hold up when the camera starts to move?\n1/8th Resolution (3 upsample passes)\nYikes! Not good at all.\nThere is a lot of flickering, and the lower we go resolution wise the worse the artifacts and flickering get\u0026hellip;\n1/16th Resolution (4 upsample passes)\nThis does make some sense\u0026hellip; If you remember one of the tradeoffs I mentioned with reducing resolution is that since pixels become larger, that means when a pixel changes it\u0026rsquo;s much more visible.\nOne of the issues I noticed when even using just upsampling is that in motion I had a lot of aliasing and instability when reducing the resolution of the lighting.\nNot good at all\u0026hellip; is there a way to fix this?\nTemporal Instability Solutions: Temporal Filtering? Fortunately the industry has come up with a way to resolve such an issue. The technique is called temporal filtering. The idea is that you use previous frames and reproject into the current frame to gradually fade in any new pixel changes that occur.\nI did go forward and implement this\u0026hellip;\n1/8th Resolution (3 upsample passes) with Full Resolution Temporal Filtering\nOk good, this definetly solves a lot of the flickering!\nBut\u0026hellip;\nIt\u0026rsquo;s still visible in some spots, and just like any solution this introduces it\u0026rsquo;s own set of problems. Problems that I find quite destructive to my final image quality.\nTemporal Filtering relies on good Temporal Resolution. In english this means that the more frames you have, the better the quality/resolve. However! In games you often don\u0026rsquo;t have the greatest temporal resolution anyway (low FPS), and it can fluctuate! So the resolve worsens especially if you are on low spec hardware! Temporal Filtering introduces ghosting/trailing artifacts. While additional attributes and conditions can be introduced to mitigate these artifacts, they will still be present in your image and you will have to constantly tweak and fight these artifacts that still muddy up your image quality in the end. 1/8th Resolution With Full Resolution Temporal Filtering (3 upsample passes)\n1/8th Resolution With Full Resolution Temporal Filtering, very visible trailing artifacts.\nPart of my attempts to fix this also was to try doing the temporal filtering at 1/8th resolution instead of at full resolution.\nAny artifacts brought on by the temporal filtering natrually would get blurred/smoothed out with the chained depth-normal aware upsampling that happens after.\nThough I also suspected that some of the flickering artifacts with the upsample would come back when we do the temporal filter at a lower resolution. Since now we can no longer fade pixel changes at a finer scale with the higher resolution to alleviate some of those flickers in the first place.\n1/8th Resolution Temporal Filtering with 3 depth-normal aware upsample passes after.\nYep, so some of the flickering artifacts have came back, that makes sense since now we are no longer doing the temporal filter at full resolution. So it can\u0026rsquo;t take care of some of the flickering artifacts brought on by the upsampling.\nAs a plus atleast it seems the sharp trailing artifacts appear to be mostly gone now, but there seems to be some other oddities going on\u0026hellip;\n1/8th Resolution With Full Resolution Temporal Filtering (3 upsample passes)\nIt appears that the image overall has gotten a lot softer. In addition also I am still seeing ghosting trails behind objects like I was before.\nThe only difference now is that it\u0026rsquo;s less sharp and more blurred out. It is an improvement over doing it at full resolution but even with some more tweaking these ghosts are still very visible and contribute to a much softer final image.\nAs a result most of these problems were enough to make me want to avoid using Temporal Filtering altogether. It ended up creating more problems than it was intended to solve so temporal filtering for me is out of the window here. There has to be another way to somehow smooth out these results without resorting to temporal filtering and the problems it creates.\nBack to the drawing board\u0026hellip;\nInvestigations into Temporal Instabilities I was geuinely curious as where this source of instability/flickering was coming from and if there was any kind of solution to solve it.\nThe global illumination technique I am using here is called Voxel Cone Tracing. I won\u0026rsquo;t explain what it does because that is not what this article is about (and it would take too long), but the algorithim itself does not introduce any inherent noise or a large amount of instability/flickering with how I have it setup.\nSo the flickering/instability has to be coming from somewhere else\u0026hellip;\nI decided to have a look at the other componets that get used to calculate the lighting. The main buffers that get used in the lighting calculations are the depth and normal buffers. If we look at the raw depth and normal buffer we are using for our inital lighting calculation, they look fine in motion.\nLeft: Full Resolution Depth | Right: Full Resolution Scene Normals ([0..1] range)\nBut wait, we are looking at the full resolution buffers here, and the lighting gets calculated at a reduced resolution. Ok, so let\u0026rsquo;s look at the buffers when it\u0026rsquo;s being used in that low resolution context directly\u0026hellip;\nLeft: 1/8th Resolution Depth | Right: 1/8th Resolution Scene Normals ([0..1] range)\nInteresting\u0026hellip; there is a lot of flickering and instability happening here, why?\nWell this is because when we are calculating lighting at a low resolution, we are using these high resolution buffers as-is. This is a problem because we can\u0026rsquo;t use it at the full resolution (we are working at 1/8th resolution) so only portions of it gets used. As a result we actually wind up skipping pixels!\nI\u0026rsquo;ll show you what I mean.\nSo here we are calculating at 1/8th resolution, when we just use the full resolution buffer as is, turns out it\u0026rsquo;s picking only a single pixel from an entire region essentially 8x8 pixel region. Out of 64 possible pixels we are just picking out only 1 pixel!\nAs a result this leads to the flickering that we are seeing, because now the changes between pixels are much more drastic leading to this increased aliasing and flickering. This is also happening in our upsampling stages as well since we are using low resolution depth/normals to check differences with the high resolution variants.\nOk, is there a way we can actually utilize those other 63 pixels that are just flat out being skipped?\nTemporal Instability Solutions: Downsampling? So before we can use our buffers in the 1/8th resolution context, we need to actually prepare them so they can be used in that 1/8th context properly. Downsampling can help us with that.\nDownsampling is about taking a large set of data, and effectively \u0026ldquo;compressing\u0026rdquo; it down to a smaller set. It\u0026rsquo;s essentially the reverse/opposite of upsampling we were doing earlier.\nThe simplest and most common downsampling algorithim used is just a simple average. Essentially you take any given number of samples, add them together and then divide by the sample count.\n1 2 4 5 4 4 4 6 5 4 7 6 6 4 (12 samples) 4 + 5 + 4 + 4 + 4 + 6 + 5 + 4 + 7 + 6 + 6 + 4 = 59 / 12 = 4.917 Notice how there are some outlier samples in the set (7) but due to the other larger quanity of numbers that are similar in range the final averaged value comes out roughly to 4 - 5.\nIn the context of pixels/images, if a lot of pixels remain roughly the same then the final value will virtually be the same, but if there is some outliers in the set the influence of those samples natrually will get reduced/averaged out. This is an interesting behavior because one of the problems I mentioned that comes from reduced resolution is that pixel changes become more visible. If the surrounding pixels are mostly the same then the pixel changes will get averaged out.\nSo it sounds like there is an inherent temporal behavior happening here\u0026hellip; Okay that seems sound\u0026hellip; why not use it for those scene buffers as well.\nIn my pipeline I introduce passes now where I downsample both scene normal buffer and depth buffer so I can prepare them for use in the low resolution lighting calculations. It\u0026rsquo;s similar to the upsampling chain setup, but I just do it in reverse now for downsampling.\nNOTE: This was my inital downsampling pipeline, however an optimization that I mentioned earlier that you can do is to pack both depth and normals into a single RGBA32 render target. This would save memory (and texture fetches later) and also simplify the pipeline here to just 1 render target that gets downsampled. Reducing draws and shader invocations, I recomend doing it.\nLeft: 1/8th Resolution Scene Depth | Right: 1/8th Resolution Scene Normals ([0..1] range)\nIn my case I just use a very simple 2x2 downsample average. Different downsampling filters could likely be utilized here but I wanted to start simple.\n1 2 3 4 5 6 7 8 float4 DownsampleAverage2x2(float2 uv, float2 texelSize) { float4 color = ColorSample(uv + float2(texelSize.x, 0)) + ColorSample(uv - float2(texelSize.x, 0)) + ColorSample(uv + float2(0, texelSize.y)) + ColorSample(uv - float2(0, texelSize.y)); return color * 0.25f; } So let\u0026rsquo;s apply this downsample filter chain and check what the normal buffer looks like in motion now\u0026hellip;\nOoooo! So after applying a downsample filter to our normal buffer (and depth buffer), much of the flickering actually vanishes when looking at the buffers by itself. This is promising\u0026hellip;\nI went ahead and applied the same downsampling process to the depth buffer as well. Now I\u0026rsquo;m left with these scene buffers downsampled to the resolution that the lighting gets calculated at, so lets use them now for calculating lighting (and the upsampling stages) and see what happens\u0026hellip; (Looking at the still frame first)\n1/8th Resolution (3 upsample passes)\nOk interesting\u0026hellip; I can actually see a lot less pixelation now compared to before even in a still frame, and the lighting results are slightly different. Some spots do appear wierder (I\u0026rsquo;ll explain why that is in a bit) but for the most part everything still looks pretty good.\nSo how does this look in motion now?\n1/8th Resolution (3 upsample passes)\nWow! Massive difference!\nWe can see this gets more dramatic also when we reduce the resolution much further and pile up more progressive upsamples. It remains pretty stable temporally, even when we are as low as 1/16th resolution!\n1/16th Resolution (4 upsample passes)\nGranted we can make out some funk in the quality. The pixelation does get more visible when we go lower, but of course that is because downsampling is not a perfect solution. Every solution introduces their own set of issues, but I find the issues here much more tame and less visible than temporal filtering artifacts like ghosting/trails that we saw before (and being bound by temporal resolution). Importantly for me, the scene remains sharp even in motion.\nStill, I want to explore why our lighting looks a little different (might even be considered worse in some areas).\nDownsampling - Drawbacks With downsampling, we reduce a set of pixels down to one, and effectively construct a fresh and unique new pixel value. Ok sure that makes sense, that is what we were trying to do in order to gain temporal stability in the first place.\nBut I want to explain the drawbacks of downsampling in this context, and why some graphics programmers shy away from doing this (and for good reason).\nThe reason being that with downsampling, you are creating a fresh new pixel out of a set of original pixels. In the context of depth, you are taking a set of original depth values, are a creating a fresh new unqiue depth value. A depth value that of which does not exist in the original depth buffer. (Same with the normal buffer)\nIf you recall back to the basic math example from earlier\u0026hellip;\n1 2 4 5 4 4 4 6 5 4 7 6 6 4 (12 samples) 4 + 5 + 4 + 4 + 4 + 6 + 5 + 4 + 7 + 6 + 6 + 4 = 59 / 12 = 4.917 The final averaged value is 4.917, which is close to 5 but 4.917 is a value that never actually existed in the original set.\nTechnically this means that if you are doing a lighting calculation for example, you use depth to reconstruct the position of a pixel so you can calculate the lighting at that point.\nWhen you introduce downsampling, that depth value actually changes so when you calculate position now, it actually gets shifted and that means the lighting at a given surface point is actually not 100% accurate. It\u0026rsquo;s in the wrong spot! Either its slightly pushed forward or slightly pushed backwards depending on the surrounding pixels of that region. In our image in some spots this can actually introduce a \u0026ldquo;halo\u0026rdquo; around some objects in the foreground.\n1/8th Resolution (3 upsample passes)\nThis also applies to the normals as well, surface normals within an area get averaged to form a new normal value. This normal value is then shifted in a different direction leading to slightly different lighting results for some surfaces.\nNow if this is a concern, there are different downsampling filters that exist. For instance with depth it\u0026rsquo;s common to actually use a min or max filter since those will pick either the closest (or farthest) pixel that was in the set and use that. However Min/Max does not average out pixel changes across an area so that inherent temporal behavior I described with averaging does not happen.\nThis is why temporal filtering is also attractive to some graphics programmers (and by extension, why some of the industry is pushing for it). It is because it does a better job at retaining \u0026ldquo;correctness\u0026rdquo; or accuracy of those original samples.\nSome even take it as far introducing \u0026ldquo;jittering\u0026rdquo;. If you remember also from earlier when I described the inital problem of how we only picked 1 pixel out of a region of 8x8 pixels\u0026hellip;\nWith jittering, we still ultimately choose 1 pixel from the region, but with the concept of time/temporal in the mix we actually change which pixel we choose within that region every frame. Across 4 frames we chose 4 different pixels from that region. Natrually the temporal filtering will blend these pixels together and we retain correctness.\nBut that is still 4 pixels out of a total of 64 in that 8x8 region, we could introduce more jitter samples across time. But covering the full 64 samples across 64 frames requires a lot of temporal resolution. Temporal resolution that often we don\u0026rsquo;t have, considering the common target framerate for games are 60 frames a second, and a lot can change in a second!\nYou could also try not dropping the resolution that far. For example just going down to a quarter resolution (rather than 1/8th) gives you a 4x4 pixel region of 4x4 = 16 pixels. Less samples that you need to cover, but still thats 16 frames of blending needed across time.\nAn interesting solution that the industry is also adopting is a combination of these two ideas so to speak. This amounts to Spatio-Temporal upsampling and this interestingly actually starts leading into the core ideas behind upscalars like FSR, DLSS, Intel XeSS. Where you have a spatial component of the filter that takes care of smoothing out pixels across an area, and the temporal component takes care of smoothing out pixel changes across time.\nHowever\u0026hellip; as I pointed out, temporal filtering at large still introduces visual issues that I\u0026rsquo;m not a fan of so I would like to avoid it if I can.\nSo ultimately I just went forward with downsampling as my main solution for temporal stability as the gains were to significant to ignore. Outside of that I don\u0026rsquo;t know any other techniques or solutions that could be employed. (NOTE: If you do have a solution or know of one, PLEASE LET ME KNOW!).\nThis doesn\u0026rsquo;t apply just to depth of course, this applies to the other scene buffers that get downsampled as well. Normals will be shifted, and with the Specular buffer those values get shifted as well.\nTemporal Solution Comparisons Left: 1/8th Resolution With Full Resolution Temporal Filter | Right: 1/8th Resolution With Downsampled Buffers\nLeft: 1/8th Resolution No Downsampled Buffers | Right: 1/8th Resolution With Downsampled Buffers\nLeft: 1/16th Resolution No Downsampled Buffers | Right: 1/16th Resolution With Downsampled Buffers\nSo to conclude this section I want to share what the pipeline looks. You might have missed it but I also mentioned that the downsampled buffers also get used in each of the upsampling stages. They are all stored in the mip levels so with each progressive upsample we use the respective mip level (that holds the downsampled render target) which helps alot with improving temproal stability in the final image. We also pointed out that it improves the spatial quality as well reducing pixelation on the final frame.\nUpsampling - How to go further? 1/8th Resolution (3 upsample passes)\nShifting gears back to improving the spatial quality of the lighting (now with the temporal component at a decent spot), even with the downsampled buffers and the best upsampling I was able to find, this still was not convincing enough for me even at lower resolutions because surfaces still get smeared too much!\nSo we need to take a step back and think as to why this is really happening. Why are the surfaces looking so smeary/blurry?\nWell when we do lighting, surface position matters a great deal. That is mostly already covered though fortunately thanks to the depth-aware upsampling that we do.\nAnother factor that significantly affects the lighting of a surface is it\u0026rsquo;s orientation, or normal. The depth-normal-aware upsampling again help with this but it\u0026rsquo;s still not enough\u0026hellip; so what\u0026rsquo;s missing?\nDo we need another scene attribute?\nWell for diffuse lighting we have the surface position, and the surface normal. We do have surface roughness and metallic, but those are meant for specular and don\u0026rsquo;t have an effect on the actual diffuse term\u0026hellip; mabye surface occlusion?\nSure lets try that. Most materials/objects in my project have an authored ambient occlusion map, let\u0026rsquo;s try factoring that in.\nFull Resolution Scene Material Occlusion\nApplying occlusion by just multiplying the final lighting buffer with it.\n1/8th Resolution (3 upsample passes) + Full Resolution Material Occlusion\nOk\u0026hellip; that helped, but it\u0026rsquo;s not good enough.\nWe can still see underneath it that things are blurred and smeary, some objects in the scene still turn into a blobby mess.\n1/8th Resolution (3 upsample passes) + Full Resolution Material Occlusion\nWhat\u0026rsquo;s worse is that some object\u0026rsquo;s in my project don\u0026rsquo;t (and wont) have authored ambient occlusion maps, so those will be left out completely!\nWe don\u0026rsquo;t have any other scene attributes we can really utilize here to push things further, so we are kind of stuck. We have to look deeper, and really examine why things are blurred and smeary.\nNow conceptually for each pixel we only have a single color that we are juggling around a given image region. We are much more selective now as to where we place this color value. But it\u0026rsquo;s only just a color value\u0026hellip; mabye we need more than just 1 color value per pixel?\nOk sure, having more color values would help, but in order to get more data shouldn\u0026rsquo;t we increase resolution to do that?\nWe can\u0026hellip; but that leads to worse performance, and that goes against what we were trying to do in the first place, which was reduce resolution and do all this upsampling!\nOk\u0026hellip; is there another way we can increase the amount of data without doing that?\nSpherical Harmonics Image by Iigo Quilez illustrating the directions that Spherical Harmonics can represent. 3 orders are represented here from top to bottom. Order 0, 1, 2, 3.\nYou may have heard or know of it, mabye not. The fancy name does tend to scare away quite a few people but it\u0026rsquo;s fairly simple in practice.\nThe general problem we are faced with is that we need to introduce more data in order to shade more accurately. We need more lighting information at a given pixel than just one single color. This is where spherical harmonics can help us.\nSpherical harmonics take a given spherical signal/data, and sum it up to a set of coefficents that you sample later. (You can think of them almost like a really small cubemap). Spherical Harmonics have orders, the more orders you have the more data/coefficents you end up with (and effectively better quality).\nI\u0026rsquo;ve set up an example in a different project here to demonstrate Spherical Harmonics. It is a simple environment that will be lit entirely with an HDRI cubemap that is projected into spherical harmonic coefficents. Starting with Order 0, graphically it\u0026rsquo;s the single top element.\nThis single float3 coefficent is the average color of the whole lighting environment. Technically this is exactly what is happening with our current lighting setup, it is just a single color we are throwing around.\nSpherical Harmonics Order 0 (1 total coefficent)\nScene Wireframe\nI shared the wireframe here to show the scene since you wouldn\u0026rsquo;t be able to make it out otherwise. But with how this scene is setup, having a single color is not enough to light these objects correctly.\nThings like occlusion of course could be used to help with shading just like we tried before\u0026hellip;\nSpherical Harmonics Order 0 (1 total coefficent) + SSAO\nBut as we also saw and learned previously\u0026hellip;\nOcclusion, regardless of what kind of flavor we may be using here (SSAO, Material Occlusion, Lightmaps, etc) is still not enough. More importantly it does not solve the underlying problem of the smeary/blurry lighting. Again this is because we are dealing with just a single color for the lighting.\nWe need more information than just a single color from our lighting environment. So lets bump the order up so we can gather more data to better shade our scene! With Order 1 we add 3 extra coefficents that are oriented to a specific direction\u0026hellip;\nSpherical Harmonics Order 1 (3 coefficents this order introduces)\nThis is now where you see the upsides to using spherical harmonics, because each coefficent now essentially provides us with more detailed information about the lighting environment\u0026hellip;\nSpherical Harmonic Coefficent 0 (L = 0, M = 0): Describes the average color of the whole lighting environment. Spherical Harmonic Coefficent 1 (L = 1, M = -1): Describes the average left/right lighting color of the environment. Spherical Harmonic Coefficent 2 (L = 1, M = 0): Describes the average top/down lighting color of the environment. Spherical Harmonic Coefficent 3 (L = 1, M = 1): Describes the average forward/back lighting color of the environment. At shading time we take the coefficents, project them reconstruct the spherical harmonic enviornment and then sample using the surface normal.\nSpherical Harmonics Order 1 (4 total coefficents)\nYou can see now we can read depth and shapes from the scene now since we are shading with more than just a single color from the environment. Depending on the orentation of the surface we get different colors that corespond to the lighting environment.\nAll of that just from 4 float3 colors!\nSpherical Harmonics Order 0 and 1 (4 total coefficents)\nNow we could go further and introduce more orders, which would give us better resolution and quality\u0026hellip; However introducing more coefficents would make things a little heavier memory wise and even a little more expensive at evaluation time.\nRemember that with each new Spherical Harmonic order you introduce a new set of coefficents that makeup that order and need to be sampled.\nWith Diffuse/Irradiance Lighting, it\u0026rsquo;s very common to at most use up to Order 1. This is because diffuse lighting tends to always be very blurry and low-frequency. Using more orders on a phenomena that is natrually blurry and low-frequency would be wasteful. (At most Order 2 is used for Diffuse/Irradiance)\nSo to keep things light and simple, we will just stick with up to Order 1 Spherical Harmonics. I would dive more into detail but just trust me on this :D\nOk fine, so how can we use it?\nSpherical Harmonics: How to apply it? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //ORDER = 0 | M = 0 float SphericalHarmonicBasis0(float3 dir) { //return 0.5f * sqrt(1.0f / PI); return 0.282094791773878f; //precomputed } //ORDER = 1 | M = -1 float SphericalHarmonicBasis1(float3 dir) { //return sqrt(3.0f / (4.0f * PI)) * dir.y; return 0.48860251190292f * dir.y; //precomputed } //ORDER = 1 | M = 0 float SphericalHarmonicBasis2(float3 dir) { //return sqrt(3.0f / (4.0f * PI)) * dir.z; return 0.48860251190292f * dir.z; //precomputed } //ORDER = 1 | M = 1 float SphericalHarmonicBasis3(float3 dir) { //return sqrt(3.0f / (4.0f * PI)) * dir.x; return 0.48860251190292f * dir.x; //precomputed } These are the spherical harmonic basis functions, but in order to use these we need to shift things a bit in our original lighting function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 float3 finalIrradiance = float3(0, 0, 0); for(int i = 0; i \u0026lt; ConeTraceDirections; i++) { //get ray direction over a HEMI-SPHERE that is oriented with the surface normal float2 random = Hammersley2dSeq(i, ConeTraceDirections); float3 vector_rayDirection = SampleHemisphereCosine(random.x, random.y, vector_sceneShadingNormals); //do cone tracing... //add final result finalIrradiance += coneTraceResult; } //average samples finalIrradiance /= ConeTraceDirections; Instead of returning/outputing the final singular color lighting based on the surface position and orientation at a given point\u0026hellip; We want to return more data/colors that describe the lighting environment at that specific point.\nWe shift things to where we switch to calculating the full 360 degree lighting environment from that point (instead of a hemisphere like before) and store that result into SH coefficents that can be sampled later.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 float3 irradianceL0 = float3(0, 0, 0); float3 irradianceL10 = float3(0, 0, 0); float3 irradianceL11 = float3(0, 0, 0); float3 irradianceL12 = float3(0, 0, 0); for(int i = 0; i \u0026lt; ConeTraceDirections; i++) { //get ray direction over a SPHERE //not a hemisphere since we want info about the whole spherical lighting environment float2 random = Hammersley2dSeq(i, ConeTraceDirections); float3 vector_rayDirection = SampleSphereUniform(random.x, random.y); //do cone tracing... //then add final results... //spherical harmonics order 0 irradianceL0 += coneTraceResult.rgb * SphericalHarmonicBasis0(vector_rayDirection); //spherical harmonics order 1 irradianceL10 += coneTraceResult.rgb * SphericalHarmonicBasis1(vector_rayDirection); irradianceL11 += coneTraceResult.rgb * SphericalHarmonicBasis2(vector_rayDirection); irradianceL12 += coneTraceResult.rgb * SphericalHarmonicBasis3(vector_rayDirection); } //average samples float scalar = (2.0f * PI / ConeTraceDirections); irradianceL0 *= scalar; irradianceL10 *= scalar; irradianceL11 *= scalar; irradianceL12 *= scalar; //you can also do other things with the SH coefficents here... //1. convolve coefficents for irradiance //2. deringing Now, in practice though with render targets and fragment shaders we typically only output 1 RGBA color.\nBut in our case we have these 4 RGB colors that we now need to return? We can\u0026rsquo;t exactly squeeze 4 RGB colors into a single output RGBA. So how can we output all the data that we need?\nI\u0026rsquo;m going to keep things brief at first, but later I explain the specifics of the packing and optimizations later here, but a short summary is that we can actually do some basic swizzling to pack the 4 RGB colors into just effectively 3 float4\u0026rsquo;s.\nThat alleviates the pressure a bit but we still end up with 3 float4s. Well we need to change things in our backend quite a bit to something that would allow us to output more data from a shader. A compute shader can do this, so we move our lighting calculation function into one and at the end of it we output the packed float4 coefficents into 3 render targets simultanoeusly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 RWTexture2D\u0026lt;float4\u0026gt; VoxelConeTraceSHA; RWTexture2D\u0026lt;float4\u0026gt; VoxelConeTraceSHB; RWTexture2D\u0026lt;float4\u0026gt; VoxelConeTraceSHC; #pragma kernel VoxelConeTracing [numthreads(THREAD_GROUP_SIZE_X, THREAD_GROUP_SIZE_Y, THREAD_GROUP_SIZE_Z)] void VoxelConeTracing(uint3 id : SV_DispatchThreadID) { //the 3 float4\u0026#39;s that we will output... float4 irradianceSHA = float4(0, 0, 0, 0); float4 irradianceSHB = float4(0, 0, 0, 0); float4 irradianceSHC = float4(0, 0, 0, 0); //calculate cone tracing like normal... //pack the spherical harmonic irradiance coefficents SphericalHarmonicsPackIrradiance_ColorL1( //input coefficents irradianceL0, irradianceL10, irradianceL11, irradianceL12, //output color irradianceSHA, irradianceSHB, irradianceSHC); //output the float4\u0026#39;s simultanoeusly VoxelConeTraceSHA[id.xyz] = irradianceSHA; VoxelConeTraceSHB[id.xyz] = irradianceSHB; VoxelConeTraceSHC[id.xyz] = irradianceSHC; } Left To Right: SHA (Render Target 0) | SHB (Render Target 1) | SHC (Render Target 2)\nI still retain all of the upsampling and filtering passes I had before, but I just do them with each of the SH render targets. (I also moved these to a compute shader)\nAfter upsampling the SH render targets to full resolution, I do an additional final pass where we sample the SH lighting environment using the full resolution scene normal, and this is the result.\n1/8th Resolution (3 upsample passes)\nWoah! We can see now that scene details are retained very well now. Normal maps and granular details are actually visible and no longer overly smeared, and all of the underlying of the scene artwork is kept in tact.\nMore importantly also the final lighting now is properly convincing at the resolution it is running at. We have full lighting information!\nLeft: 1/8th Resolution No SH + Occlusion | Right: 1/8th Resolution with SH\nThat pipe I pointed out initally, well now we can make it out pretty well!\nWe can take the occlusion term we had previously and multiply it with our final upsampled spherical harmonic lighting to get the final results.\n1/8th Resolution + Full Resolution Occlusion (3 upsample passes)\nThis is currently the best I\u0026rsquo;ve been able to come up with in regards to getting very good and usable results with very low resolution lighting input. Remember, this is all coming from lighting that is calculated at 1/8th screen resolution which originally would have looked like this.\n1/8th Resolution Raw (No Upsampling/Downsampling)\nThe best part also is that we can actually drop the resolution even lower, and details will still be mostly retained.\nNOTE: These screenshots don\u0026rsquo;t have occlusion term factored in.\nLeft: 1/16th Resolution No Spherical Harmonics (4 upsample passes) | Right: 1/16th Resolution With Spherical Harmonics (4 upsample passes)\nLeft: 1/32th Resolution No Spherical Harmonics (5 upsample passes) | Right: 1/32th Resolution With Spherical Harmonics (5 upsample passes)\nSpherical Harmonic Optimizations: Compute Shader With my inital setup with spherical harmonics, I ran the lighting calculation fragment shader effectively 3 times for each of the spherical harmonic render targets. In addition the upsampling passes would also get multiplied since each of SH render targets needed upsampling to full resolution.\nFor inital prototyping and proof of concept it worked fine, but this definetly was not ideal and had some performance degredations.\nOne of the things I did to alleviate this was to move both the main lighting calculation, and the upsampling pass into compute shader kernels.\nThat way I just execute the main lighting compute shader kernel once, and that would allow me to simultanoeusly write to the 3 SH render targets that I have (Rather than needing to run the lighting calculation fragment shader 3 times). The same also for the upsampling kernel, simultaneously upsampling the 3 render targets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 RWTexture2D\u0026lt;float4\u0026gt; VoxelConeTraceSHA; RWTexture2D\u0026lt;float4\u0026gt; VoxelConeTraceSHB; RWTexture2D\u0026lt;float4\u0026gt; VoxelConeTraceSHC; #pragma kernel VoxelConeTracing [numthreads(THREAD_GROUP_SIZE_X, THREAD_GROUP_SIZE_Y, THREAD_GROUP_SIZE_Z)] void VoxelConeTracing(uint3 id : SV_DispatchThreadID) { //the 3 float4\u0026#39;s that we will output... float4 irradianceSHA = float4(0, 0, 0, 0); float4 irradianceSHB = float4(0, 0, 0, 0); float4 irradianceSHC = float4(0, 0, 0, 0); //calculate cone tracing like normal... //pack the spherical harmonic irradiance coefficents SphericalHarmonicsPackIrradiance_ColorL1( //input coefficents irradianceL0, irradianceL10, irradianceL11, irradianceL12, //output color irradianceSHA, irradianceSHB, irradianceSHC); //output the float4\u0026#39;s simultanoeusly VoxelConeTraceSHA[id.xyz] = irradianceSHA; VoxelConeTraceSHB[id.xyz] = irradianceSHB; VoxelConeTraceSHC[id.xyz] = irradianceSHC; } Spherical Harmonic Optimizations: Render Target Packing I also mentioned that I had 3 SH render targets. Which might not make sense considering we are dealing with Order 1 Spherical Harmonics which leaves you with 4 float3\u0026rsquo;s.\nRGBA Render Target 0 R: L0.r G: L0.g B: L0.b A: (Unused) RGBA Render Target 1 R: L1X.r G: L1X.g B: L1X.b A: (Unused) RGBA Render Target 2 R: L1Y.r G: L1Y.g B: L1Y.b A: (Unused) RGBA Render Target 3 R: L1Z.r G: L1Z.g B: L1Z.b A: (Unused) You could have 4 render targets, but that is a bit wasteful memory wise. Turns out we can just pack those 4 float3\u0026rsquo;s into 3 float4\u0026rsquo;s with some basic swizzling. (moving each of the components around)\nNOTE: The render targets are RGBA64. Each component is 16 bit half precison, not 8 bit because we are dealing with lighting. We need HDR for proper shading results.\nRGBA Render Target 0 R: L0.r G: L0.g B: L0.b A: L1X.r RGBA Render Target 1 R: L1X.g G: L1X.b B: L1Y.r A: L1Y.g RGBA Render Target 2 R: L1Y.b G: L1Z.r B: L1Z.g A: L1Z.b Now currently I still utilize 3 Render Targets, but this can actually be taken much further as you can do some clever packing techniques to where you can shrink things down to 2 Render Targets.\nTrading color accuracy for luminance, in this case the first Order 0 coefficent remains as is, but Order 1 coefficents store luminance rather than a full RGB color.\nRGBA Render Target 0: (L0.r, L1.g, L1.b, L1X) R: L0.r G: L0.g B: L0.b A: L1X RGHalf Render Target 1: (L1Y, L1Z) R: L1Y G: L1Z There are likely other clever packing schemes that exist, you are welcome to try them and if it works for you, then do it! (If you also know of any other additional techniques for this please let me know! I\u0026rsquo;m still on the lookout)\nSpherical Harmonic Bonuses Another bonus of said effects is that since we have information on the lighting environment beyond just a single color now for every pixel.\nWith spherical harmonics we can actually do a trick to where we can calculate the dominant direction of light from the SH environment. This dominant direction vector can then be used to derive a specular highlight term that can enhance the quality of our global illumination reflection buffer!\nThe dominant direction also in theory can also be used for potentially more things (like localized contact shadows, or micro shadows, etc).\nFuture: How to go even further and beyond? Where to go from here of course?\nAt this point I am mostly satisified with the results, but I am still actively exploring techniques or other avenues to improve results even further.\nA big part of global illumination is specular/reflections. We only looked at diffuse/irradiance here, but often reflections are handled in a seperate pass. SSR is a potential solution here (and a common industry one) but that has obvious drawbacks with only being a screen-space effect. It would be ideal if both diffuse and specular/reflections could be done once in a single pass.\nAt the time of writing I have my eyeballs set on radiance caching with octahedron maps.\nSpherical harmonics are great but don\u0026rsquo;t contain enough information to store radiance (or reflection). Having effectively a low resolution cubemap per pixel (or area) would be more useful. That is where octahedral mapping can come in. That octahedron map can be used for reflections directly, and diffuse/irradiance can also be derived from it using spherical harmonics just like we did before.\nThe only issue with the approach is that memory wise it will be considerably heavier, needing to store effectively a cubemap per pixel. Potentially more expensive also since now you are juggling alot more data, and you need very good probe interpolation across pixels.\nI do have a prototype working, but it needs alot more work before I\u0026rsquo;d consider it to be usable and viable. Probe interpolation currently being a difficult problem to solve well.\nReferences / Sources List of references that helped with my implementations and understanding.\n(theinstructionlimit) gaussian-blur-revisited-part-one\n(theinstructionlimit) gaussian-blur-revisited-part-two\n(bartwronski) bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset\n(c0de517e) downsampled-effects-with-depth-aware\n(hikiko-blog) depth_aware_upsampling_experiments\n(patapom) SHPortal\nBy: David Matos\n","date":"2026-01-07T00:00:00Z","image":"https://frostbone25.github.io/p/adventures-global-illumination-downscaling/content/final-rendered-image_hu_2df95da0962aef60.png","permalink":"https://frostbone25.github.io/p/adventures-global-illumination-downscaling/","title":"Global Illumination Downscaling"},{"content":"By: David Matos\nMy adventures into improving Box Projected Specular Reflections!\nYou can find the Github Repository Here!\nNOTE: In-case it isn\u0026rsquo;t obvious, most of the examples/images shown here are of a pure metallic/reflective material to show the effects more clearly. Specifically the material is configured to be fully metallic, with smoothness/perceptualRoughness at 0.5 or 50%.\nTable of Contents Preface Contact Hardening Contact Hardening: None Contact Hardening: Approximated Contact Hardening: HDRP Approximation / Frostbite Engine Contact Hardening: Godot Approximation Contact Hardening: Raytraced Contact Hardening: Raytracing Options Static Noise (Non-Deterministic Sampling) Animated Noise (Non-Deterministic Sampling) Deterministic Sampling Beveled Box Projection Box Based Specular Occlusion Credits / Sources Preface Cubemap Reflections are a very common and simple way to provide reflection/specular data to a set of objects in a scene (or a specific area within a scene).\nA probe is placed at a specific point, and at runtime (or during baking) a camera captures the scene at 6 different orientations (90 degrees, within a square).\nGraphic from LearnOpenGL\nEach of the faces of this \u0026ldquo;cube\u0026rdquo; are combined into a cubemap texture asset. Then this texture asset goes through an additional process step called \u0026ldquo;specular convolution\u0026rdquo; which effectively generates a convolved or \u0026ldquo;blurred\u0026rdquo; version of the cubemap that coresponds with \u0026ldquo;rougher\u0026rdquo; reflections that get stored in the mips of the texture. (This is important so when the cubemap reflection gets sampled, objects with rougher surfaces can have blurrier reflections)\nMip 0 Mip 1 Mip 2 Mip 3 Mip 4 Mip 5 This then can be fed to objects within an Axis-Aligned Bounding Box (AABB) Volume inside a scene to sample environment reflections.\nLeft: No Normal Mapping | Right: Normal Mapped\nHowever, as Physically Based Rendering and Shading became standard, reflections are a huge part of how an object is shaded now. Sampling this cubemap for environment reflections as-is is not convincing enough in most cases.\nThese reflection probes are usually authored to be placed using an Axis-Aligned Bounding Box Volume (AABB), to tell objects within the volume to use a specific reflection cubemap.\nNow fortunately the industry devised a clever way to improve upon the quality of these reflections by projecting the reflection vector onto a box-shape. The box shape in question is the very same AABB volume used to define the reflection volume in the first place, this is parallax-corrected cubemaps.\n1 2 3 4 5 6 7 8 9 10 //compute reflection vector float3 vector_reflectionDirection = reflect(-vector_viewDirection, vector_normalDirection); //project reflection vector onto box vector_reflectionDirection = BoxProjectedCubemapDirection( vector_reflectionDirection, //the original reflection vector to project vector_worldPosition, //world position vector_reflectionProbePosition, //the reflection probe position vector_reflectionProbeBoxMin, //reflection probe AABB min vector_reflectionProbeBoxMax); //reflection probe AABB max Left: Box Projected + No Normal Mapping | Right: Box Projected + Normal Mapped\nHowever, there is an issue with this approach\u0026hellip;\nWhen sampling the reflection, we choose the mip level based on surface roughness. Meaning that the reflection has a consistent mip level (or sharpness) wherever it is being sampled from within the volume.\nThis assumption works fine for non-parallax corrected cubemaps (because the volume is essentially infinitely large), but factoring in parallax correction where we have a finite-sized volume this assumption starts to fall apart.\n1 2 3 float perceptualRoughness = 1.0 - _Smoothness; float mip = perceptualRoughness * MAX_REFLECTION_PROBE_MIP_LEVEL; //\u0026lt;--- this is not enough anymore! When we look at the real world, we notice that when reflections are closer to the source they remain relatively sharp. The further it goes the blurrier/rougher it gets. This phenomena is not accounted for when sampling our own reflection probes.\nSo when sampling the reflection cubemap, the mip level should vary not just based on surface roughness, but also depending on how close we are to the source.\nContact Hardening Contact Hardening: None Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //Unity\u0026#39;s Default Box Projection function inline float3 UnityBoxProjectedCubemapDirectionDefault(float3 worldRefl, float3 worldPos, float4 cubemapCenter, float4 boxMin, float4 boxMax) { float3 nrdir = normalize(worldRefl); float3 rbmax = (boxMax.xyz - worldPos) / nrdir; float3 rbmin = (boxMin.xyz - worldPos) / nrdir; float3 rbminmax = (nrdir \u0026gt; 0.0f) ? rbmax : rbmin; float dist = min(min(rbminmax.x, rbminmax.y), rbminmax.z); worldPos -= cubemapCenter.xyz; worldRefl = worldPos + nrdir * dist; return worldRefl; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 //in object fragment shader... //compute reflection vector float3 vector_reflectionDirection = reflect(-vector_viewDirection, vector_normalDirection); //if box projection is enabled, modify our vector to project reflections onto a world space box (defined by the reflection probe) vector_reflectionDirection = UnityBoxProjectedCubemapDirectionDefault(vector_reflectionDirection, vector_worldPosition, unity_SpecCube0_ProbePosition, unity_SpecCube0_BoxMin, unity_SpecCube0_BoxMax); //remap our smoothness parameter to PBR roughness float perceptualRoughness = 1.0 - _Smoothness; float roughness = perceptualRoughness * perceptualRoughness; //compute the cubemap mip level based on perceptual roughness //NOTE: in unity this is perceptualRoughness * UNITY_SPECCUBE_LOD_STEPS float mip = UnityPerceptualRoughnessToMipmapLevel(perceptualRoughness); //sample the environment reflection enviormentReflection = UNITY_SAMPLE_TEXCUBE_LOD(unity_SpecCube0, vector_reflectionDirection.xyz, mip); enviormentReflection.rgb = DecodeHDR(enviormentReflection, unity_SpecCube0_HDR); This is the usual way to sample box-projected cubemap reflections. The issue as I described though is that the roughness of the reflection is consistent across the entire volume regardless of camera position, and object position. This is not the case when you compare it to the the real world, or a proper raytraced/ground truth result.\nWith that there are a few implementations I explored that attempt to alleviate that problem and make them more true to life and higher fidelity.\nContact Hardening: Approximated Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //Unity\u0026#39;s Default Box Projection function, tweaked to output hit distance inline float3 UnityBoxProjectedCubemapDirectionDefault(float3 worldRefl, float3 worldPos, float4 cubemapCenter, float4 boxMin, float4 boxMax, out float distanceToHitPoint) { float3 nrdir = normalize(worldRefl); float3 rbmax = (boxMax.xyz - worldPos) / nrdir; float3 rbmin = (boxMin.xyz - worldPos) / nrdir; float3 rbminmax = (nrdir \u0026gt; 0.0f) ? rbmax : rbmin; distanceToHitPoint = min(min(rbminmax.x, rbminmax.y), rbminmax.z); worldPos -= cubemapCenter.xyz; worldRefl = worldPos + nrdir * distanceToHitPoint; return worldRefl; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 //in object fragment shader... //compute reflection vector float3 vector_reflectionDirection = reflect(-vector_viewDirection, vector_normalDirection); //remap our smoothness parameter to PBR roughness float perceptualRoughness = 1.0 - _Smoothness; float roughness = perceptualRoughness * perceptualRoughness; //offical roughness term for pbr shading //compute the cubemap mip level based on perceptual roughness //NOTE: this is basically perceptualRoughness * UNITY_SPECCUBE_LOD_STEPS float mipOriginal = UnityPerceptualRoughnessToMipmapLevel(perceptualRoughness); //this will store the \u0026#34;intersectionDistance\u0026#34; result from the box projection float intersectionDistance = 0; //if box projection is enabled, modify our vector to project reflections onto a world space box (defined by the reflection probe) vector_reflectionDirection = UnityBoxProjectedCubemapDirectionDefault(vector_reflectionDirection, vector_worldPosition, unity_SpecCube0_ProbePosition, unity_SpecCube0_BoxMin, unity_SpecCube0_BoxMax, intersectionDistance); //added a clamp to the mip offset, helps to make sure that when a fragment is far away the mip level doesn\u0026#39;t climb to a high value and look wierd //this tends to look much better when clamped intersectionDistance = clamp(intersectionDistance, 0.0f, UNITY_SPECCUBE_LOD_STEPS); //compute new mip level based on the intersectionDistance value (this is mostly arbitrary) float mip = lerp(0.0f, mipOriginal, intersectionDistance / UNITY_SPECCUBE_LOD_STEPS); //sample the provided reflection probe at the given mip level enviormentReflection = UNITY_SAMPLE_TEXCUBE_LOD(unity_SpecCube0, vector_reflectionDirection.xyz, mip); enviormentReflection.rgb = DecodeHDR(enviormentReflection, unity_SpecCube0_HDR); This is achieved by modifying the existing box projection method to output a hit distance from the bounds of the box, to where the current fragment/pixel is.\nThis hit distance is then used to offset the mip level when sampling the cubemap to be sharper when closer to the bounds of the probe, and rougher when farther from the bounds. It\u0026rsquo;s very cheap and fast, though not accurate as it fails to model anisotropic reflections (Isotropic, or specular elongation).\nDespite that, this alleviates a couple of visual problems we saw before.\nThis attempts to solve the problem of reflections being too consistent across the entire volume, making them look closer to a proper raytraced result. (I.e the closer to the actual source of the reflection bounds, the sharper the reflection gets) In most situations this contact hardening actually contributes to improved specular occlusion. Since the contact hardening reflections reveal the underlying cubemap more, there is less of a glowing appearance in the corners of the bounds. (NOTE: This is assuming your reflection probe is placed and configured well to approximate the geometry of the space/room, if it\u0026rsquo;s not this might also contribute to light leaking) This is a marginal improvement over the classic method, and it\u0026rsquo;s really cheap!\nNOTE: The approximation is mostly arbitrarily tweaked by hand with random values until it looks right, or otherwise known as \u0026ldquo;banging random rocks together until we get something decent\u0026rdquo;. In addition, any possible tricks to help with mimicking anisotropic reflections would help here.\nContact Hardening: HDRP Approximation / Frostbite Engine 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 //From Moving Frostbite to PBR document //This function fakes the roughness based integration of reflection probes by adjusting the roughness value //NOTE: Untouched from HDRP float ComputeDistanceBaseRoughness(float distanceIntersectionToShadedPoint, float distanceIntersectionToProbeCenter, float perceptualRoughness) { float newPerceptualRoughness = clamp(distanceIntersectionToShadedPoint / distanceIntersectionToProbeCenter * perceptualRoughness, 0, perceptualRoughness); return lerp(newPerceptualRoughness, perceptualRoughness, perceptualRoughness); } //This simplified version assume that we care about the result only when we are inside the box //NOTE: Untouched from HDRP float IntersectRayAABBSimple(float3 start, float3 dir, float3 boxMin, float3 boxMax) { float3 invDir = rcp(dir); // Find the ray intersection with box plane float3 rbmin = (boxMin - start) * invDir; float3 rbmax = (boxMax - start) * invDir; float3 rbminmax = float3((dir.x \u0026gt; 0.0) ? rbmax.x : rbmin.x, (dir.y \u0026gt; 0.0) ? rbmax.y : rbmin.y, (dir.z \u0026gt; 0.0) ? rbmax.z : rbmin.z); return min(min(rbminmax.x, rbminmax.y), rbminmax.z); } //return projectionDistance, can be used in ComputeDistanceBaseRoughness formula //return in R the unormalized corrected direction which is used to fetch cubemap but also its length represent the distance of the capture point to the intersection //Length R can be reuse as a parameter of ComputeDistanceBaseRoughness for distIntersectionToProbeCenter //NOTE: Modified to be much simpler, and to work with the Built-In Render Pipeline (BIRP) float EvaluateLight_EnvIntersection(float3 worldSpacePosition, inout float3 R) { float projectionDistance = IntersectRayAABBSimple(worldSpacePosition, R, unity_SpecCube0_BoxMin.xyz, unity_SpecCube0_BoxMax.xyz); R = (worldSpacePosition + projectionDistance * R) - unity_SpecCube0_ProbePosition.xyz; return projectionDistance; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 //in object fragment shader... //compute reflection vector float3 vector_reflectionDirection = reflect(-vector_viewDirection, vector_normalDirection); float perceptualRoughness = 1.0 - _Smoothness; //use HDRP method to compute the intersection distance (and also box project vector_reflectionDirection) float projectionDistance = EvaluateLight_EnvIntersection(vector_worldPosition, vector_reflectionDirection); //use HDRP formula to calculate roughness based on distance float distanceBasedRoughness = ComputeDistanceBaseRoughness(projectionDistance, length(vector_reflectionDirection), perceptualRoughness); //the output of distanceBasedRoughness is the new perceptual roughness perceptualRoughness = distanceBasedRoughness; float roughness = perceptualRoughness * perceptualRoughness; //compute the cubemap mip level based on perceptual roughness float mip = UnityPerceptualRoughnessToMipmapLevel(distanceBasedRoughness); //sample the provided reflection probe at the given mip level enviormentReflection = UNITY_SAMPLE_TEXCUBE_LOD(unity_SpecCube0, vector_reflectionDirection.xyz, mip); enviormentReflection.rgb = DecodeHDR(enviormentReflection, unity_SpecCube0_HDR); This I found about recently, where Unity HDRP had a very similar implementation to the original approximation I stumbled upon. (The implmentation also stems from improvements made to the Frostbite engine)\nThe difference being that the approximation used here is less obvious/extreme than the other original approximation I had, and it tries to stay somewhat physically plausible.\nComparing it to ground truth it is closer in some cases than the other, for a very similar cost. A big improvement over the classic method, and it\u0026rsquo;s really cheap!\nContact Hardening: Godot Approximation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //Unity\u0026#39;s Default Box Projection function, tweaked to output hit distance inline float3 UnityBoxProjectedCubemapDirectionDefault(float3 worldRefl, float3 worldPos, float4 cubemapCenter, float4 boxMin, float4 boxMax, out float distanceToHitPoint) { float3 nrdir = normalize(worldRefl); float3 rbmax = (boxMax.xyz - worldPos) / nrdir; float3 rbmin = (boxMin.xyz - worldPos) / nrdir; float3 rbminmax = (nrdir \u0026gt; 0.0f) ? rbmax : rbmin; distanceToHitPoint = min(min(rbminmax.x, rbminmax.y), rbminmax.z); worldPos -= cubemapCenter.xyz; worldRefl = worldPos + nrdir * distanceToHitPoint; return worldRefl; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 //in object fragment shader... //calculate a basic fresnel term (cheap) float fresnelTerm = 1.0f - max(dot(vector_normalDirection, vector_viewDirection), 0.0f); //note: higher power means that you effectively need to be closer to the surface for the contact hardening to appear fresnelTerm = pow(fresnelTerm, _ApproximationGodotFresnelPower); //fresnelTerm = fresnelTerm * fresnelTerm; //hardcoded pow 2.0 //fresnelTerm = fresnelTerm * fresnelTerm * fresnelTerm; //hardcoded pow 3.0 //fresnelTerm = fresnelTerm * fresnelTerm * fresnelTerm * fresnelTerm; //hardcoded pow 4.0 (this was the chosen value in the original godot improvement) //compute reflection vector float3 vector_reflectionDirection = reflect(-vector_viewDirection, vector_normalDirection); //remap our smoothness parameter to PBR roughness float perceptualRoughness = 1.0 - _Smoothness; float roughness = perceptualRoughness * perceptualRoughness; //compute the cubemap mip level based on perceptual roughness //NOTE: this is basically perceptualRoughness * UNITY_SPECCUBE_LOD_STEPS float mipOriginal = UnityPerceptualRoughnessToMipmapLevel(perceptualRoughness); //this will store the \u0026#34;intersectionDistance\u0026#34; result from the box projection float intersectionDistance = 0; //if box projection is enabled, modify our vector to project reflections onto a world space box (defined by the reflection probe) vector_reflectionDirection = UnityBoxProjectedCubemapDirectionDefault(vector_reflectionDirection, vector_worldPosition, unity_SpecCube0_ProbePosition, unity_SpecCube0_BoxMin, unity_SpecCube0_BoxMax, intersectionDistance); //remap distance to be relative to amount of mips. float reflectionRoughness = intersectionDistance / UNITY_SPECCUBE_LOD_STEPS; //reflection roughness shifts depending on the fresnel term reflectionRoughness *= 1.0 - fresnelTerm; //increase roughness when viewing angle is perpendicular to avoid overly sharp reflections on rough surfaces. reflectionRoughness += (1.0 - fresnelTerm) * perceptualRoughness; reflectionRoughness = saturate(reflectionRoughness); //ensures fully rough materials don\u0026#39;t have reflection contact hardening. float mipMin = (roughness * roughness) * UNITY_SPECCUBE_LOD_STEPS; float mip = lerp(mipMin, mipOriginal, reflectionRoughness); //sample the provided reflection probe at the given mip level enviormentReflection = UNITY_SAMPLE_TEXCUBE_LOD(unity_SpecCube0, vector_reflectionDirection.xyz, mip); enviormentReflection.rgb = DecodeHDR(enviormentReflection, unity_SpecCube0_HDR); This is a recent development I\u0026rsquo;ve been following. As a result of this repository the folks with Godot Engine have been implementing this feature to improve their reflections rendering.\nGodot Proposal 11670 Godot Pull 106292 Their implementation is based on my original approximation but a fresnel term has been added as an additional attribute (beyond just surface position/distance) to offset the mip level of the reflection. This has been checked against Blender Cycles renderer for ground-truth comparisons, and accuracy is much improved.\nApperance wise the effect is more muted and less pronounced, but when at grazing angles the effect starts to become more noticable.\nIf we check the Godot approximation against my inital one we can see that the reflections get sharper when we get closer to the surface (thanks to the fresnel term) where with my original approximation it does not.\nLeft: Approximation | Right: Godot Approximation with Fresnel\nLeft: Approximation + Normal Mapping | Right: Godot Approximation with Fresnel + Normal Mapping\nAn additional improvement/optimization to this would be utilizing the actual PBR fresnel term that normally gets calculated and multiplied with your environment reflection, rather than just calculating a new fresnel term for this specific instance.\nOnce again, a big improvement over the classic method, and it\u0026rsquo;s cheap!\nContact Hardening: Raytraced Code can be found in here\u0026hellip;\nReflectionTracing.cginc ImprovedBoxProjectedReflections.shader This is the most accurate method out of the previous ones we had. This is equivalent to ground truth, and unlike the approximations it can properly model anisotropic specular (specular elongation). It\u0026rsquo;s the most accurate and best looking one. However in comparison it is more complicated and slower than previous methods and requires alot of samples to mitigate artifacts (More on this later).\nIt\u0026rsquo;s a massive improvement over the classic method and the approximations, but it\u0026rsquo;s more complicated and expensive (Again more on this later, it\u0026rsquo;s not as bad as you think)\nContact Hardening Comparisons (Non-Normal Mapped Material) Left To Right: None, Approximation, HDRP Approximation, Godot Approximation, Raytraced\nContact Hardening Comparisons (Normal Mapped Material) Left To Right: None, Approximation, HDRP Approximation, Godot Approximation, Raytraced\nContact Hardening: Raytracing Options Earlier we mentioned that while this method is the most accurate and high quality, it\u0026rsquo;s also expensive when compared to it\u0026rsquo;s approximated counterparts. I also want to note that it\u0026rsquo;s important also to keep in mind the \u0026ldquo;raytracing\u0026rdquo; being done here is fairly simple compared to what one might expect, it\u0026rsquo;s just AABB ray testing against the reflection probe bounds. It\u0026rsquo;s all software-based and can even run on a Quest 2 standalone at a decent framerate. With that said there are a few of things we can do here to try to help both improve quality and performance.\nThe current implementation also is using an Importance Sampled GGX Function for specular convolution, at 64 samples.\nThe blue noise also being used later in the examples/project is a precomputed 3D Texture with 64 slices (64 iterations). The 3D texture being built from 64 slices of precomputed blue noise textures from Moments in Graphics by Christoph Peters. I do also have precomputed Spatio-Temporal Blue Noise (STBN) volumes in the project that I also combined, however for some reason the result\u0026rsquo;s I got were worse than with the ones from Christoph Peters. Might be something I did wrong or incorrectly, someone let me know please!\nStatic Noise (Non-Deterministic Sampling) Left: Static White Noise | Right: Static Blue Noise\nBy default white noise is being used. In the current implementation we can use blue-noise sampling to try to improve the perceptual quality. Comparing side by side the difference is small in a static scenario like this, but the blue noise is noticably more consistent and pleasing compared to white noise. At lower sample counts obviously the blue noise shows it\u0026rsquo;s strengths more but of course lower samples = lots of noise regardless.\nOne other thing also featured in the shader is a \u0026ldquo;Mip Offset\u0026rdquo; cheat. As it turns out the approximations we have earlier actually work great here and work as a way to blur the results the further the fragment/pixel is. This is something that can be done but in my opinon I would caution against it. Reason being that it alters the reflection properties by blurring the underlying reflection a little too much, and you will start to lose out on that specular enlongation.\nLeft: Blue Noise | Right: Static Blue Noise With Mip Offsetting\nAnimated Noise (Non-Deterministic Sampling) Left: Animated White Noise | Right: Animated Blue Noise\nOne common solution we can try here is to animate the noise, and accumulate samples over time. This scenario is also where blue noise shines the most, since we are using 1 sample per pixel for every frame, and the sample pattern changes every frame. Using Unity\u0026rsquo;s Default TAA Here and with a stationary camera the results get cleaned up very nicely, and it\u0026rsquo;s performant as it\u0026rsquo;s only 1 sample per pixel, accumlated across multiple frames.\nNow obviously the drawback of this solution is that it is temporal, it accumulates results over multiple frames. Because of that you\u0026rsquo;ll have to deal with the problems that come about with temporal rendering such as disocclusion events, changing perspective, ghosting, and that whole mess. Filtering this with an additional blur that is normal aware will help you here but that introduces inaccuracies as well with trying to retain sharp reflections. Pick your poison.\nDeterministic Sampling If you don\u0026rsquo;t like noise, you can choose to forgo Monte-Carlo random sampling and instead go for a more uniform/deterministic sampling type. You swap artifacts from noise to aliasing, just like the noise however the more samples you have the better. Better performance in theory (better cache coherence) but can look pretty bad.\nLeft: Deterministic Sampling | Right: Deterministic Sampling With Mip Offset\nI also introduced a comparison with the enabled \u0026ldquo;Mip Offsetting\u0026rdquo; approximation which can be used in conjunction with this deterministic sampling approach to get you usable results on smooth surfaces. However just like I mentioned earlier, introducing this does overly blur the reflection and you lose out on that specular elongation.\nBeveled Box Projection Beveled Box Projection: Off Classic box projection, it\u0026rsquo;s fast and simple, but has sharp edges and can look jarring at grazing angles especially when the cubemap is sampled at a higher mip level.\nBeveled Box Projection: On Using a beveled box projection with an adjustable bevel factor, to smooth out the sharp edges you\u0026rsquo;d get with the classic method.\nThe initial idea here is\nGive an artist-controlled parameter to smooth out the reflection probe projection. A more complex but optically inspired approach by mapping the bevel factor to increased roughness, so the blurrier the reflection gets, the smoother the edges of the box projection are. Granted the current implementation has artifacts at high bevel values, note the top left and right sides. You also have to deal with added complexity/instruction counts for a bevel box projection.\nIn my opinion: For most circumstances this is not as transformative as contact hardening, which already visually \u0026ldquo;un-boxes\u0026rdquo; the appearance of your box projected reflection for a much cheaper cost.\nBox Based Specular Occlusion Another oddball thing we can do, since we have the information of an environment with a box shape. We can use the hit distance from that to do a form of specular occlusion. The closer a pixel/fragment is to the bounds of the box, the more occluded it becomes. So in circumstances where push comes to shove you need to occlude specular or reflections, this is a possible route.\nBox Occlusion: Off Using the classic box projected reflections, note the glowing apperance where the floor meets the walls.\nBox Occlusion: On With box occlusion, there is an added darkening that is introduced. Eliminating the glowing apperance.\nNOTE: The material here is a pure metallic/reflective material so the results here can look pretty rough. In cases where you have non-metallic materials this can work well for specular occlusion. It depends on your project/use cases.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //recompute reflection direction float3 vector_specularOcclusionReflectionDirection = reflect(-vector_viewDirection, vector_normalDirection); //This is the output hit distance, but we will use it as our specular occlusion factor float specularOcclusionFactor = 0; vector_specularOcclusionReflectionDirection = UnityBoxProjectedCubemapDirectionDefault(vector_specularOcclusionReflectionDirection, vector_worldPosition, unity_SpecCube0_ProbePosition, unity_SpecCube0_BoxMin, unity_SpecCube0_BoxMax, specularOcclusionFactor); //artistic parameters for specular occlusion specularOcclusionFactor = pow(specularOcclusionFactor, _ExperimentalSpecularOcclusionPower); specularOcclusionFactor *= _ExperimentalSpecularOcclusionMultiplier; //clamp so we stay within 0..1 specularOcclusionFactor = saturate(specularOcclusionFactor); //apply to environment reflection enviormentReflection *= lerp(1.0f, specularOcclusionFactor, _ExperimentalSpecularOcclusionIntensity); TODO/Ideas: Raytraced: Reflection-Luminance Importance Sampling Bevel Box Projection: Fix Artifacts Credits / Sources Inigo Quilez: Beveled Box Code Christoph Peters: Free blue noise textures. Unity HDRP: Approximated Distance Based Roughness Implementation Unity SRP Core: Various Functions Unity.cginc: Unity Box Projection Cactus_On_Fire: Inital inspiration George: Tipping me off to Unity HDRP\u0026rsquo;s implementation Calinou: Improved Godot version of Contact Hardening Approximation. lander-vr: Improved Godot version of Contact Hardening Approximation. clayjohn: Improved Godot version of Contact Hardening Approximation. By: David Matos\n","date":"2026-01-06T00:00:00Z","image":"https://frostbone25.github.io/p/adventures-box-projected-reflections/5-box-projectedGodotA_hu_87353c49316ad93e.png","permalink":"https://frostbone25.github.io/p/adventures-box-projected-reflections/","title":"Box Projected Cubemap Reflections"},{"content":"By: David Matos\nPreface So you have your Spherical Harmonic coefficents generated\u0026hellip; Cool! Now let\u0026rsquo;s say you are developing a system where you want to use spherical harmonics to retain spherical information across a wide area.\nGood examples of this are a light probe system for determining the diffuse shading of an object at a specific position. Another one is visibility information to determine if something is occluded or not.\nThe important thing to keep in mind here is that, especially depending on the amount of spherical harmonic orders you are using, and the context they are applied in, you may need to store a little or a lot.\nThe solution to handling large stacks of data is to reduce the size of it. Done either with compression or reducing precison. This is something I wanted to experiment with and explore as I have not seen any material online covering this, nor showing the consequences of compression in the context of spherical harmonics.\nIn my case I am exploring spherical harmonics in the context of holding diffuse light information so I require 3 components (Red, Green, Blue / RGB) and I need to cover a value range beyond 1.0.\nNow in a different example, like storing visibility information, you usually only need 1 component and a value range between 0 and 1 so the requirements are different. (And would like to point out that consequences of less data precison would likely be less impactful here)\nNow depending on how you handle lighting in your engine/setup the range of light information can vary widely. In my case the range of light values with these HDRIs I\u0026rsquo;m testing go up to 5 at it\u0026rsquo;s highest. Now in some engines/setups if you are using a full HDR range with physical light units you may have value ranges that skyrocket past 5 so keep that in mind because you\u0026rsquo;ll need a datatype that can handle that full range or more.\nSome Notes before we begin\u0026hellip;\nI tested signed values along with unsigned since spherical harmonic projection can usually generate coefficents with negative values, and this is usually refered to as ringing. So even results with unsigned were shown because I wanted to have a visual example of how it mangles things, but don\u0026rsquo;t forget about this phenomena when using spherical harmonics.\nSince we are in a lighting context it\u0026rsquo;s possible that you can use HDR value compression techniques here, however I have not in these tests.\nIn addition to reducing data precison/compression, depending on how you go about it you will most likely be trading processing cost for disk/memory usage. Keep this in mind and make sure to profile your project.\nLets start by showing the highlights of the experiment (the rest of the result\u0026rsquo;s will be shown later).\nFloat32 [4 Bytes | 32 bits] (Original) We have 2 Order Spherical Harmonic Irradiance data. This requires a set of 9 float3\u0026rsquo;s (RGB) coefficents. 9 float3\u0026rsquo;s is 27 floats total.\nWe also have 6 Order Spherical Harmonic Radiance data. This requires a set of 49 float3\u0026rsquo;s (RGB) coefficents. 49 float3\u0026rsquo;s is 147 floats total.\nThere are 4 bytes in a float so the sizes come about to\u0026hellip;\n1 2 2nd Order Irradiance/Diffuse = 4 * 27 = 108 bytes. 6th Order Radiance/Reflection = 4 * 147 = 588 bytes. 108 bytes to store diffuse/irradiance spherical lighting information, and 588 bytes to store reflection/radiance spherical lighting information. Both with floating point precison, which yields a very large value range that is also signed. This is fairly standard but we can do better!\nHalf16 [2 Bytes | 16 bits] A Half16 is 2 bytes, so that yields\u0026hellip;\n1 2 2nd Order Irradiance/Diffuse = 2 * 27 = 54 bytes. 6th Order Radiance/Reflection = 2 * 147 = 294 bytes. The size/precison of the data is literally half of a float. This is a fairly common thing to do when floating point precison is not required and the values are still signed.\nChecking the results visually, we can see that pretty much in all circumstances everything looks identical. Keep in mind that our light value ranges don\u0026rsquo;t exceed 5.0 so your results may vary.\nSigned 10 Bit Decimal 2 [4 Bytes | 32 bits for 3 RGB Values] Lets go further, and this is what I was most curious to see. How does the reconstruction fall apart when there is a lack of precison.\nNote that this will approach the territory where you need to watch for your value ranges. My computed coefficent ranges for the HDRIs I\u0026rsquo;ve tested against were [-0.5f - 5.0f] roughly.\nSo I chose to reduce the values down to a signed 10 bit value. Which affords you [-512, 511] value range. In my testing I also found that atleast 2 decimal places were needed to keep results looking fairly close to the original.\nSo with some basic compression math the actual range would be [-5.12, 5.11]. You can pack these small 10 bit values into a full 32 bit (4 byte) value. (R 10 | G 10 | B 10 | 2 unused). You could also try something similar to D3Ds R11G11B10 format often used for HDR to squeeze a little extra data for some of the channels, but I wanted to keep things consistent here.\nSo 3 floats effectively compressed into 4 bytes, so that yields\u0026hellip;\n1 2 2nd Order Irradiance/Diffuse = 4 * 9 = 36 bytes. 6th Order Radiance/Reflection = 4 * 49 = 196 bytes. This is about as low as we can get it in theory while trying to respect the HDR values we have. If we go any lower then we will just mangle the data (you can see this happening later)\nI should note that these examples are the most basic and simple way to reduce/compress these kind of values. However I mentioned earlier that this boils down to effectively compressing HDR color data since we are in a lighting context. So if you wanted to get fancy you could employ concepts behind HDR compression i.e. like using RGBM, RGBE, or even LogLUV to get better results. But you need to keep in mind that depending on the data reduction/compression methods used, you will trade memory/space with processing power.\nNow I\u0026rsquo;ll show the results with the many different variations I have\u0026hellip;\nFull Data Reduction Results By: David Matos\n","date":"2025-07-15T00:00:00Z","image":"https://frostbone25.github.io/p/adventures-spherical-harmonics-coefficent-precision/preview-screenshot_hu_62c9ca427e7b800a.png","permalink":"https://frostbone25.github.io/p/adventures-spherical-harmonics-coefficent-precision/","title":"Spherical Harmonics Coefficent Precision"},{"content":"This is my introduction!\n","date":"2023-03-06T00:00:00Z","image":"https://frostbone25.github.io/p/introduction/cover_hu_9e6bcf9cfe9a9448.jpg","permalink":"https://frostbone25.github.io/p/introduction/","title":"Welcome!"}]